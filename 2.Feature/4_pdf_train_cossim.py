# pdfë¬¸ì„œì˜ ë¬¸ì¥ë“¤ê³¼, train dataì˜ ë¬¸ì¥ë“¤ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ ê°€ì¥ ì°¸ê³ í• ë§Œí•œ pdfë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.

import os
import pandas as pd
import torch
from sentence_transformers import SentenceTransformer, util
import re

# ğŸ“‚ íŒŒì¼ ê²½ë¡œ ì„¤ì •
script_dir = os.path.dirname(os.path.abspath(__file__))
train_file_path = os.path.join(script_dir, "..", "1.Data", "train_cleaned.csv")  # train ë°ì´í„°
pdf_processed_file_path = os.path.join(script_dir, "..", "1.Data", "PDF_ë¬¸ì¥_ì²˜ë¦¬.csv")  # ë¬¸ì¥ ë‹¨ìœ„ PDF ë°ì´í„°

# ğŸ”¹ Sentence-BERT ëª¨ë¸ ë¡œë“œ
device = "cuda" if torch.cuda.is_available() else "cpu"
model = SentenceTransformer("jhgan/ko-sbert-sts", device=device)

# ğŸ”¹ ë°ì´í„° ë¡œë“œ
train_df = pd.read_csv(train_file_path)
pdf_df = pd.read_csv(pdf_processed_file_path)

# ğŸ”¹ PDF íŒŒì¼ì„ ë¬¸ì„œ ë‹¨ìœ„ë¡œ ê²°í•© (ë¬¸ì¥ â†’ íŒŒì¼ë³„ í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°)
pdf_grouped = pdf_df.groupby("íŒŒì¼ëª…")["ë¬¸ì¥"].apply(lambda x: " ".join(x)).reset_index()

# âœ… **Step 1: train ì§ˆë¬¸ì—ì„œ `''` ì•ˆì˜ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ**
def extract_keywords(text):
    """ì§ˆë¬¸ì—ì„œ `''` ì•ˆì˜ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ"""
    return re.findall(r"'(.*?)'", text)

train_df["í‚¤ì›Œë“œ"] = train_df["question"].apply(extract_keywords)

# âœ… **Step 2: PDFì—ì„œ í‚¤ì›Œë“œê°€ ê³¨ê³ ë£¨ í¬í•¨ëœ ë¬¸ì„œ ì„ íƒ**
def compute_keyword_coverage_score(pdf_text, keywords):
    """PDF ë¬¸ì„œì—ì„œ í‚¤ì›Œë“œê°€ ì–¼ë§ˆë‚˜ ê³¨ê³ ë£¨ í¬í•¨ë˜ì—ˆëŠ”ì§€ ì ìˆ˜í™”"""
    unique_matches = set()
    total_matches = 0

    for word in keywords:
        if word in pdf_text:
            unique_matches.add(word)  # í¬í•¨ëœ í‚¤ì›Œë“œ ê°œìˆ˜ ê³„ì‚°
            total_matches += pdf_text.count(word)  # ì „ì²´ ë“±ì¥ íšŸìˆ˜

    if len(keywords) == 0:
        return 0

    score = len(unique_matches) / len(keywords)  # í‚¤ì›Œë“œ ê· í˜• ì ìˆ˜ (0~1)
    return score

def find_best_pdfs_for_question(keywords):
    """í‚¤ì›Œë“œê°€ ê³¨ê³ ë£¨ í¬í•¨ëœ PDF íŒŒì¼ 3~5ê°œ ì„ íƒ"""
    pdf_scores = {}

    for _, row in pdf_grouped.iterrows():
        pdf_text = row["ë¬¸ì¥"]
        score = compute_keyword_coverage_score(pdf_text, keywords)
        if score > 0:
            pdf_scores[row["íŒŒì¼ëª…"]] = score  # PDF íŒŒì¼ë³„ ì ìˆ˜ ì €ì¥

    return sorted(pdf_scores, key=pdf_scores.get, reverse=True)[:5]

train_df["ê´€ë ¨ PDF íŒŒì¼"] = train_df["í‚¤ì›Œë“œ"].apply(find_best_pdfs_for_question)

# âœ… **Step 3: ì„ íƒëœ PDF íŒŒì¼ì—ì„œ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ ë¶„ì„**
def extract_relevant_sentences(pdf_df, pdf_filenames, keywords):
    """PDFì—ì„œ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ 3~5ê°œ ì¶”ì¶œ"""
    relevant_sentences = []
    
    for _, row in pdf_df.iterrows():
        if row["íŒŒì¼ëª…"] in pdf_filenames:
            for keyword in keywords:
                if keyword in row["ë¬¸ì¥"]:
                    relevant_sentences.append((row["íŒŒì¼ëª…"], row["ë¬¸ì¥"]))

    return relevant_sentences[:5]  # ìµœëŒ€ 5ê°œ ë¬¸ì¥ ë°˜í™˜

# âœ… **Step 4: ì„ íƒëœ PDF íŒŒì¼ê³¼ ì§ˆë¬¸ ì „ì²´ë¥¼ ë¹„êµí•˜ì—¬ ìœ ì‚¬ë„ ë¶„ì„**
matched_results = []
total_questions = len(train_df)

print(f"ğŸ”¥ ì „ì²´ {total_questions}ê°œì˜ ì§ˆë¬¸ì„ ë¶„ì„ ì‹œì‘...")

for idx, row in train_df.iloc[:200].iterrows():
    question = row["question"]
    keywords = row["í‚¤ì›Œë“œ"]
    related_pdfs = row["ê´€ë ¨ PDF íŒŒì¼"]

    # ğŸ”¹ ì§„í–‰ë¥  ì¶œë ¥
    if idx % 50 == 0:
        print(f"ğŸ”„ ì§„í–‰ ì¤‘: {idx}/{total_questions} ({(idx/total_questions)*100:.2f}%) ì™„ë£Œ")

    # ğŸ”¹ ì„ íƒëœ PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
    selected_pdf_texts = pdf_grouped[pdf_grouped["íŒŒì¼ëª…"].isin(related_pdfs)]["ë¬¸ì¥"].tolist()
    selected_pdf_filenames = pdf_grouped[pdf_grouped["íŒŒì¼ëª…"].isin(related_pdfs)]["íŒŒì¼ëª…"].tolist()

    # ğŸ”¹ ì„ íƒëœ PDFê°€ ì—†ìœ¼ë©´ íŒ¨ìŠ¤
    if not selected_pdf_texts:
        continue

    # ğŸ”¹ ì§ˆë¬¸ ì„ë² ë”© ìƒì„±
    question_embedding = model.encode(question, convert_to_tensor=True, device=device)

    # ğŸ”¹ PDF ë¬¸ì„œ(íŒŒì¼ ë‹¨ìœ„) ì„ë² ë”© ìƒì„±
    pdf_embeddings = model.encode(selected_pdf_texts, convert_to_tensor=True, device=device)

    # ğŸ”¹ ìœ ì‚¬ë„ ê³„ì‚°
    similarities = util.pytorch_cos_sim(question_embedding, pdf_embeddings)[0]

    # ğŸ”¹ ê°€ì¥ ìœ ì‚¬í•œ PDF ì°¾ê¸°
    best_match_idx = similarities.argmax().item()
    best_match_text = selected_pdf_texts[best_match_idx]
    best_match_pdf = selected_pdf_filenames[best_match_idx]
    best_score = similarities[best_match_idx].item()

    # ğŸ”¹ **í•´ë‹¹ PDFì—ì„œ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì£¼ìš” ë¬¸ì¥ 3~5ê°œ ì¶”ì¶œ**
    top_matched_sentences = extract_relevant_sentences(pdf_df, [best_match_pdf], keywords)

    # âœ… ê²°ê³¼ ì €ì¥
    matched_results.append([question, keywords, best_match_pdf, best_match_text, best_score, top_matched_sentences])

print("âœ… ëª¨ë“  ì§ˆë¬¸ ë¶„ì„ ì™„ë£Œ!")

# âœ… **ê²°ê³¼ ì €ì¥**
result_df = pd.DataFrame(matched_results, columns=["ì§ˆë¬¸", "í‚¤ì›Œë“œ", "PDF íŒŒì¼ëª…", "ìœ ì‚¬í•œ PDF ì „ì²´ ë‚´ìš©", "ìœ ì‚¬ë„", "PDF ë‚´ ê´€ë ¨ ë¬¸ì¥"])
output_csv = os.path.join(script_dir, "..", "1.Data", "ìœ ì‚¬ë„_ë§¤ì¹­_ë¬¸ë§¥ë¶„ì„_ì „ì²´.csv")
result_df.to_csv(output_csv, index=False, encoding="utf-8-sig")

print(f"âœ… ë¬¸ë§¥ ë¶„ì„ ì™„ë£Œ! ì €ì¥ ìœ„ì¹˜: {output_csv}")
